{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science - House Price Prediction By Example "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the libraries\n",
    "  - It is good practise to consolidate the import statements in one place to understand what needs to be availble in order to run the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loading and reviewing the datasets\n",
    "  - Several datasets are loaded including datasets for training and testing.\n",
    "  - Calling `DataFrame#head` and `DataFrame#tail` on the dataset is encouraged to understand what's contained in the dataset and if there is unwanted data or information on the bottom of the DataFrame like notes etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Finding `null`/`na` values\n",
    "  - columns may or may not contain empty values (`null`, `na`, `nan`)\n",
    "  - as features (predictors/regressors) may be required to predict a value, an understanding of the quality of the data is crucial for creating a model, as it might lead to an (unwanted) reduction of the depending on the model\n",
    "  - by executing `DataFrame.isnull().sum().sort_values(ascending=False)`, a list of features is printed by the number of `null` values in descending order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploring the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When talking to the Pandas API, the `DataFrame` datatype is usually whats used to interact with the data. Understanding DataFrames is required for a basic interaction with the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame#describe\n",
    "\n",
    "The `describe` function is used to give a first overview of the data and can be called on a DataFrame or one of its columns\n",
    "\n",
    "| Property   | Description |\n",
    "|------------|--------------|\n",
    "| count      | The number of entries with real values |\n",
    "| mean       | The mean value of a column             |\n",
    "| std        | The standard deviation                 |\n",
    "| min        | The min value of a row                 |\n",
    "| 25%        | The value of the 25% percentile        |\n",
    "| 50%        | The value of the 50% percentile        |\n",
    "| 75%        | The value of the 50% percentile        |\n",
    "| max        | The max value of a row                 |\n",
    "\n",
    "##### Standard Deviation\n",
    "\n",
    "A low value in the `std` mean, the data is spread close to the mean, while a high `std` indicates a lot of variance in the data with very small and very high values, compared to the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding The Domain - Inspecting the target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `SalePrice` column is an interesting column as it is the \"target\" of our prediction. `desfribe` can be called on a DataFrame column, which returns a summary of the column just as it did for the whole DataFrame (/table). By inspecting the data, we could reason about the datasets validity (which generally requires some knowlege of the domain were operating in)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explorative Data Analysis With Visualizations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matplotlib Histogram\n",
    "\n",
    "By plotting the price in relation to the count of houses, the general distribution of houses in certain price ranges gets visualized. In the given notebook a dataset is loaded with most houses in a price range from 100000 USD to 300000 USD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scipy.stats - Statistics\n",
    "\n",
    "From `scipy.stats` the `norm` is imported and used. The norm function is used for:\n",
    "```\n",
    "fit(data, loc=0, scale=1)\n",
    "    Parameter estimates for generic data.\n",
    "```\n",
    "The function returns data for the location and the scale â­¢ mean and std?\n",
    "\n",
    "> The fit method of the distributions can be used to estimate the parameters of the distribution, and the test is repeated using probabilities of the estimated distribution.\n",
    "\n",
    "##### Question\n",
    "Why did we do it and render a normal distribution based on the calculated values? Was it done for comparison reasons with the graph we rendered for `SalesPrice`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Features\n",
    "\n",
    "Categorical data can be understood - at least in terms of frequency - by simply using a bar chart. Other features could be explored and understood by other visualizations, i.e. scatter plots. This is done to reduce the data to the features necessary to make a valid prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Correlations Between Features\n",
    "\n",
    "Features might be redundant if one feature influences a second feature, so we're searching for features which are highly correlated with the `SalesPrice`. Plotting a heatmap shows some very dark spots which indicate features, which relate strongly with the `SalesPrice`.\n",
    "\n",
    "By plotting correlation matrices with `k` variables, identifying independed variables gets more reliable. The seaborn heatmap plots the correlation coefficient in the crossing cell. By comparing the resulting values and appying domain knowledge, important and probably redundant features can be isolated or removed.\n",
    "\n",
    "Seaborn also allows to do draw a similar matrix by plotting graphs in each cell, which also gives a good overview.\n",
    "\n",
    "___Using this matrix, we're also lookig for patterns between the features.___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By applying linear regression to various independent, we can inspect the accuracy for each predictor independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R-squared\n",
    "\n",
    "R-squared is used to explain variability of the target variable. The higher the value, to more likely it is that the value can be used to model a prediction for the target value.\n",
    "\n",
    "### $\\hat{\\beta}_0$\n",
    "\n",
    "> If we don't have any Overallquality (`OverallQual`) factor in the picture, we would expect Saleprice (y) to be -9.621 exponentially raised to power of 4. In other words it is very small.\n",
    "\n",
    "Does that mean, that `OverallQual` is a major contributing factor which contributes to the overall magnitude of the target variable?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:nf] *",
   "language": "python",
   "name": "conda-env-nf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
